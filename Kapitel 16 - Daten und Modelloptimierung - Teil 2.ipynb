{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "f10b6a54",
      "metadata": {
        "id": "f10b6a54"
      },
      "source": [
        "# Daten und Modelloptimierung - Teil 2"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5ec0d1da",
      "metadata": {
        "id": "5ec0d1da"
      },
      "source": [
        "## Quelle der Daten\n",
        "\n",
        "https://www.kaggle.com/datasets/uciml/breast-cancer-wisconsin-data (zuletzt aufgerufen: 01/2024)\n",
        "\n",
        "https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+%28Diagnostic%29 (zuletzt aufgerufen: 01/2024)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "22c2c65d",
      "metadata": {
        "id": "22c2c65d"
      },
      "source": [
        "## Installation der Bibliotheken"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0b334b7d"
      },
      "source": [
        "# Installieren der keras_tuner-Bibliothek\n",
        "%pip install keras_tuner"
      ],
      "id": "0b334b7d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa79e439",
      "metadata": {
        "id": "aa79e439"
      },
      "outputs": [],
      "source": [
        "# Bibliotheken importieren\n",
        "import pandas as pd\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "import keras_tuner\n",
        "\n",
        "from sklearn.model_selection import StratifiedKFold"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4a3d36f5",
      "metadata": {
        "id": "4a3d36f5"
      },
      "source": [
        "## Einlesen der Daten"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "34224e40",
      "metadata": {
        "id": "34224e40"
      },
      "outputs": [],
      "source": [
        "# Daten laden und fehlende Werte entfernen\n",
        "data_url = \"https://github.com/timwgnd/Lehrbuch-Kuenstliche-Intelligenz-in-der-Medizin/raw/refs/heads/main/Brustkrebs.xlsx\"\n",
        "data = pd.read_excel(io=data_url, sheet_name = \"Tabelle1\")\n",
        "\n",
        "# Entfernen von Zeilen mit fehlenden Werten\n",
        "data = data.dropna()\n",
        "\n",
        "# Anzeigen der ersten Zeilen des DataFrames, um einen Überblick über die Daten zu erhalten\n",
        "print(data.head().to_markdown(index=False, tablefmt='psql'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "31b8e8db",
      "metadata": {
        "id": "31b8e8db"
      },
      "outputs": [],
      "source": [
        "# Diagnose-Werte in numerische Werte umwandeln\n",
        "diagnosis_new = {\"benign\": 0, \"malignant\": 1}\n",
        "\n",
        "data[\"diagnosis\"] = data[\"diagnosis\"].replace(diagnosis_new)\n",
        "\n",
        "print(data.head().to_markdown(index=False, tablefmt='psql'))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e04b5ba7",
      "metadata": {
        "id": "e04b5ba7"
      },
      "source": [
        "## Aufteilung der Daten"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "429964c9",
      "metadata": {
        "id": "429964c9"
      },
      "outputs": [],
      "source": [
        "# Daten in Features (x) und Zielvariable (y) aufteilen\n",
        "x = data.iloc[:, 1:]\n",
        "\n",
        "y = data.iloc[:, 0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6cb87e4e",
      "metadata": {
        "id": "6cb87e4e"
      },
      "outputs": [],
      "source": [
        "# Anzeigen der ersten Zeilen des Feature-Datensatzes (x)\n",
        "print(x.head().to_markdown(index=False, tablefmt='psql'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d088e07d",
      "metadata": {
        "id": "d088e07d"
      },
      "outputs": [],
      "source": [
        "# Anzeigen der ersten Zeilen der Zielvariablen (y)\n",
        "print(y.head().to_markdown(index=False, tablefmt='psql'))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Anzeigen der ersten Zeilen der Zielvariablen (y)\n",
        "print(y.value_counts().to_markdown(tablefmt='psql'))"
      ],
      "metadata": {
        "id": "jCDk0bn-j_KP"
      },
      "id": "jCDk0bn-j_KP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "59ab009b",
      "metadata": {
        "id": "59ab009b"
      },
      "source": [
        "## Principal Component Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ad723a41",
      "metadata": {
        "id": "ad723a41"
      },
      "outputs": [],
      "source": [
        "# Skalieren der Feature-Daten (x) mit StandardScaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "scaler.fit(x)\n",
        "\n",
        "scaled_data = scaler.transform(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ab137d11",
      "metadata": {
        "id": "ab137d11"
      },
      "outputs": [],
      "source": [
        "# Anwenden der Principal Component Analysis (PCA) mit\n",
        "# 15 Komponenten auf die skalierten Daten\n",
        "pca = PCA(n_components = 15)\n",
        "\n",
        "pca.fit(scaled_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dee83a18",
      "metadata": {
        "id": "dee83a18"
      },
      "outputs": [],
      "source": [
        "# Erstellen eines Scree Plots zur Visualisierung der erklärten Varianz durch\n",
        "# die Hauptkomponenten der PCA mit 15 Komponenten\n",
        "PC_values = np.arange(pca.n_components_) + 1\n",
        "\n",
        "plt.plot(PC_values, pca.explained_variance_ratio_, \"o-\", linewidth = 2)\n",
        "plt.xticks(np.arange(1, len(PC_values)+1, 1))\n",
        "\n",
        "plt.title(\"Scree Plot\")\n",
        "plt.xlabel(\"Principal Component\")\n",
        "plt.ylabel(\"Variance Explained\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0abc8b2b",
      "metadata": {
        "id": "0abc8b2b"
      },
      "outputs": [],
      "source": [
        "# Anwenden der Principal Component Analysis (PCA) mit 3 Komponenten auf die\n",
        "# skalierten Daten, basierend auf dem Scree Plot\n",
        "pca = PCA(n_components = 3)\n",
        "\n",
        "pca.fit(scaled_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "de34d6f9",
      "metadata": {
        "id": "de34d6f9"
      },
      "outputs": [],
      "source": [
        "# Erstellen eines Scree Plots für die PCA mit 3 Komponenten zur Bestätigung der Varianz\n",
        "PC_values = np.arange(pca.n_components_) + 1\n",
        "\n",
        "plt.plot(PC_values, pca.explained_variance_ratio_, \"o-\", linewidth = 2)\n",
        "plt.xticks(np.arange(1, len(PC_values)+1, 1))\n",
        "\n",
        "plt.title(\"Scree Plot\")\n",
        "plt.xlabel(\"Principal Component\")\n",
        "plt.ylabel(\"Variance Explained\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b8245275",
      "metadata": {
        "id": "b8245275"
      },
      "outputs": [],
      "source": [
        "# Transformieren der skalierten Daten mit der PCA, reduziert auf 3 Hauptkomponenten\n",
        "x_pca = pca.transform(scaled_data)\n",
        "\n",
        "print(x_pca)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4eb15061",
      "metadata": {
        "id": "4eb15061"
      },
      "outputs": [],
      "source": [
        "# Visualisieren der ersten beiden Hauptkomponenten (PC1 und PC2) der\n",
        "# PCA-transformierten Daten, eingefärbt nach der Diagnose\n",
        "plt.figure(figsize = (8,6))\n",
        "\n",
        "plt.scatter(x_pca[:,0],x_pca[:,1], c = data[\"diagnosis\"])\n",
        "\n",
        "plt.xlabel(\"PC1\")\n",
        "plt.ylabel(\"PC2\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6f301ef3",
      "metadata": {
        "id": "6f301ef3"
      },
      "outputs": [],
      "source": [
        "# Aufteilen der PCA-transformierten Daten und der Zielvariablen in Trainings- und Testsets\n",
        "# für das Modelltraining\n",
        "x_train, x_test, y_train, y_test = train_test_split(x_pca, y, test_size = 0.15)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3626a722",
      "metadata": {
        "id": "3626a722"
      },
      "source": [
        "## Erstellen, Trainieren und Evaluieren des KI-Modells"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4c3c7669",
      "metadata": {
        "id": "4c3c7669"
      },
      "outputs": [],
      "source": [
        "# Erstellen eines ersten neuronalen Modells\n",
        "model_1 = tf.keras.models.Sequential()\n",
        "\n",
        "model_1.add(tf.keras.layers.Dense(64, activation = tf.nn.relu))\n",
        "model_1.add(tf.keras.layers.Dense(256, activation = tf.nn.relu))\n",
        "model_1.add(tf.keras.layers.Dense(128, activation = tf.nn.relu))\n",
        "\n",
        "model_1.add(tf.keras.layers.Dense(2, activation = tf.nn.softmax))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f766e6b0",
      "metadata": {
        "id": "f766e6b0"
      },
      "outputs": [],
      "source": [
        "# Kompilieren des ersten Modells mit SGD-Optimizer und Sparse Categorical Crossentropy-Loss\n",
        "model_1.compile(\n",
        "    optimizer = \"SGD\",\n",
        "    loss = \"sparse_categorical_crossentropy\",\n",
        "    metrics = [\"accuracy\"]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b6290c7d",
      "metadata": {
        "id": "b6290c7d"
      },
      "outputs": [],
      "source": [
        "# Trainieren des ersten Modells mit den Trainingsdaten für 10 Epochen\n",
        "model_1.fit(x_train, y_train, epochs = 10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5c572ae0",
      "metadata": {
        "id": "5c572ae0"
      },
      "outputs": [],
      "source": [
        "# Evaluieren des ersten Modells mit den Testdaten\n",
        "eval_results = model_1.evaluate(x_test, y_test)\n",
        "print(\"[test loss, test accuracy]:\", eval_results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3c44242f",
      "metadata": {
        "id": "3c44242f"
      },
      "outputs": [],
      "source": [
        "# Erstellen des zweiten neuronalen Modells mit Regularisierung\n",
        "model_2 = tf.keras.models.Sequential()\n",
        "\n",
        "model_2.add(tf.keras.layers.Dense(64, activation = tf.nn.relu,\n",
        "                                kernel_initializer = \"he_uniform\",\n",
        "                                kernel_regularizer = tf.keras.regularizers.L1(0.01),\n",
        "                                bias_regularizer = tf.keras.regularizers.L2(0.01)))\n",
        "model_2.add(tf.keras.layers.Dense(256, activation = tf.nn.relu))\n",
        "model_2.add(tf.keras.layers.Dense(128, activation = tf.nn.relu))\n",
        "\n",
        "model_2.add(tf.keras.layers.Dense(2, activation = tf.nn.softmax))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B-PopbSIlG3_"
      },
      "outputs": [],
      "source": [
        "# Kompilieren des zweiten Modells mit SGD-Optimizer und Sparse Categorical Crossentropy-Loss\n",
        "model_2.compile(\n",
        "    optimizer = \"SGD\",\n",
        "    loss = \"sparse_categorical_crossentropy\",\n",
        "    metrics = [\"accuracy\"]\n",
        ")"
      ],
      "id": "B-PopbSIlG3_"
    },
    {
      "cell_type": "code",
      "source": [
        "# Trainieren des zweiten Modells mit den Trainingsdaten für 5 Epochen\n",
        "model_2.fit(x_train, y_train, epochs = 10)"
      ],
      "metadata": {
        "id": "iwxvieGwlGrT"
      },
      "id": "iwxvieGwlGrT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluieren des zweiten Modells mit den Testdaten\n",
        "eval_results = model_2.evaluate(x_test, y_test)\n",
        "print(\"[test loss, test accuracy]:\", eval_results)"
      ],
      "metadata": {
        "id": "3iK8SD8zlJ4o"
      },
      "id": "3iK8SD8zlJ4o",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "8bb9c801",
      "metadata": {
        "id": "8bb9c801"
      },
      "source": [
        "## Hyperparameter-Optimierung mit Random Search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d38ad64",
      "metadata": {
        "id": "9d38ad64"
      },
      "outputs": [],
      "source": [
        "# Funktion zum Erstellen eines Keras-Modells für die Hyperparameter-Optimierung\n",
        "def create_model(hp):\n",
        "    model = tf.keras.models.Sequential()\n",
        "\n",
        "    # Definiere zu testende Hyperparameter-Einstellungen\n",
        "    hp_units = hp.Int(\"units\", min_value = 64, max_value = 128, step = 32)\n",
        "    hp_loss = hp.Choice(\"loss\", [\"sparse_categorical_crossentropy\", \"MSE\"])\n",
        "    hp_optimizer = hp.Choice(\"optimizer\", [\"adam\", \"SGD\"])\n",
        "\n",
        "    # Eingangsform basierend auf der Anzahl der PCA-Komponenten festlegen\n",
        "    model.add(tf.keras.layers.Dense(units=hp_units, activation = tf.nn.relu, input_shape=(3,)))\n",
        "    model.add(tf.keras.layers.Dense(256, activation = tf.nn.relu))\n",
        "    model.add(tf.keras.layers.Dense(units=hp_units, activation = tf.nn.relu))\n",
        "\n",
        "    model.add(tf.keras.layers.Dense(2, activation = tf.nn.softmax))\n",
        "\n",
        "    model.compile(optimizer = hp_optimizer, loss = hp_loss, metrics = [\"accuracy\"])\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6f95d9ef",
      "metadata": {
        "id": "6f95d9ef"
      },
      "outputs": [],
      "source": [
        "# Erstellen eines Random-Search Tuners\n",
        "tuner = keras_tuner.RandomSearch(\n",
        "    hypermodel=create_model,\n",
        "    objective='val_accuracy',\n",
        "    max_trials=5\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "81c55d62",
      "metadata": {
        "id": "81c55d62"
      },
      "outputs": [],
      "source": [
        "# Durchführen der Random Search zur Hyperparameter-Optimierung des Modells\n",
        "tuner.search_space_summary()\n",
        "tuner.search(x_train, y_train, epochs = 10, validation_split = 0.2)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ermitteln des besten Hyperparameter-Settings\n",
        "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
        "\n",
        "print(best_hps.values)"
      ],
      "metadata": {
        "id": "-0cOPtIpvUNq"
      },
      "id": "-0cOPtIpvUNq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Erstellen eines finalen Modells mit den besten Hyperparametern\n",
        "# und anschließendes Trainieren auf den Daten für 50 Epochen\n",
        "model_tuned = tuner.hypermodel.build(best_hps)\n",
        "history = model_tuned.fit(x_train, y_train, epochs=50, validation_split=0.2)\n",
        "\n",
        "val_acc_per_epoch = history.history['val_accuracy']\n",
        "best_epoch = val_acc_per_epoch.index(max(val_acc_per_epoch)) + 1\n",
        "print('Best epoch: %d' % (best_epoch,))"
      ],
      "metadata": {
        "id": "_rvYINfHvTTV"
      },
      "id": "_rvYINfHvTTV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotten der Trainings- und Testgenauigkeit\n",
        "plt.plot(model_tuned.history.history[\"accuracy\"], label = \"train_accuracy\")\n",
        "plt.plot(model_tuned.history.history[\"val_accuracy\"], label = \"test_accuracy\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "5W308gbBwxlD"
      },
      "id": "5W308gbBwxlD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "39069b9a",
      "metadata": {
        "id": "39069b9a"
      },
      "outputs": [],
      "source": [
        "# Evaluieren des finalen Modells mit den Testdaten\n",
        "eval_results = model_tuned.evaluate(x_test, y_test)\n",
        "print(\"[test loss, test accuracy]:\", eval_results)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}