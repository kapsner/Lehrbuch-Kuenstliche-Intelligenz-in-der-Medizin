{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc2bdc5e",
   "metadata": {},
   "source": [
    "# Graph Neural Networks (GNNs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "829c0913",
   "metadata": {},
   "source": [
    "## Quelle der Daten\n",
    "\n",
    "https://www.kaggle.com/datasets/tawsifurrahman/covid19-radiography-database?resource=download (zuletzt aufgerufen 01/2024)\n",
    "    \n",
    "M.E.H. Chowdhury, T. Rahman, A. Khandakar, R. Mazhar, M.A. Kadir, Z.B. Mahbub, K.R. Islam, M.S. Khan, A. Iqbal, N. Al-Emadi, M.B.I. Reaz, M. T. Islam, “Can AI help in screening Viral and COVID-19 pneumonia?” IEEE Access, Vol. 8, 2020, pp. 132665 - 132676.\n",
    "\n",
    "Rahman, T., Khandakar, A., Qiblawey, Y., Tahir, A., Kiranyaz, S., Kashem, S.B.A., Islam, M.T., Maadeed, S.A., Zughaier, S.M., Khan, M.S. and Chowdhury, M.E., 2020. Exploring the Effect of Image Enhancement Techniques on COVID-19 Detection using Chest X-ray Images."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a84e0f",
   "metadata": {},
   "source": [
    "## Installation der Bibliotheken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8789a50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import cv2\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import Model, layers\n",
    "\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "\n",
    "import keras\n",
    "\n",
    "import requests\n",
    "\n",
    "import tempfile\n",
    "\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed883e2",
   "metadata": {},
   "source": [
    "## Download der Daten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b76629",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset from online resource, given here: https://www.kaggle.com/datasets/tawsifurrahman/covid19-radiography-database\n",
    "ds_url = \"https://drive.usercontent.google.com/download?id=1bum9Sehb3AzUMHLhBMuowPKyr_PCrB3a&export=download&confirm=1\"\n",
    "temp_dir = tempfile.mkdtemp()\n",
    "print(f\"Temporary file-directory for saving dataset: '{temp_dir}'\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d82fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for downloading from google-drive, use method described elsewhere (https://stackoverflow.com/a/39225272)\n",
    "# with some modifications\n",
    "\n",
    "def download_file_from_google_drive(URL, destination_dir):\n",
    "\n",
    "    session = requests.Session()\n",
    "\n",
    "    response = session.get(URL, stream=True)\n",
    "    token = get_confirm_token(response)\n",
    "\n",
    "    if token:\n",
    "        params = {\"confirm\": token}\n",
    "        response = session.get(URL, params=params, stream=True)\n",
    "\n",
    "    save_response_content(response, destination_dir)\n",
    "\n",
    "\n",
    "def get_confirm_token(response):\n",
    "    for key, value in response.cookies.items():\n",
    "        if key.startswith(\"download_warning\"):\n",
    "            return value\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "def save_response_content(response, destination_dir):\n",
    "    CHUNK_SIZE = 32768\n",
    "\n",
    "    with open(destination_dir + \"/data.zip\", \"wb\") as f:\n",
    "        for chunk in response.iter_content(CHUNK_SIZE):\n",
    "            if chunk:  # filter out keep-alive new chunks\n",
    "                f.write(chunk)\n",
    "\n",
    "\n",
    "download_file_from_google_drive(ds_url, temp_dir)\n",
    "\n",
    "print(\"Folder structure inside 'tempdir':\\n\")\n",
    "print(os.listdir(temp_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6a86fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract zip-file\n",
    "shutil.unpack_archive(filename=temp_dir + \"/data.zip\", extract_dir=temp_dir)\n",
    "print(\"Folder structure inside 'tempdir':\\n\")\n",
    "print(os.listdir(temp_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69bcb19f",
   "metadata": {},
   "source": [
    "## Einlesen und Präprozessierung der Daten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c3d0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_path = temp_dir + \"/COVID-19_Radiography_Dataset\"\n",
    "print(os.listdir(main_path))\n",
    "\n",
    "covid_dir = os.path.join(main_path, \"COVID/images\")\n",
    "normal_dir = os.path.join(main_path, \"Normal/images\")\n",
    "\n",
    "print(\"Anzahl Bilder mit COVID:\", len(os.listdir(covid_dir)))\n",
    "print(\"Anzahl normaler Bilder:\", len(os.listdir(normal_dir)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb2b59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_image = cv2.imread(os.path.join(covid_dir, \"COVID-1.png\"))\n",
    "\n",
    "plt.imshow(example_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08cbfd1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b7026b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadImages(dir, size, label):\n",
    "  images = []\n",
    "  labels = []\n",
    "\n",
    "  for i in range(len(size)):\n",
    "    img_path = dir + \"/\" + size[i]\n",
    "    img = cv2.imread(img_path)\n",
    "    img = img / 255.0\n",
    "    img = cv2.resize(img, (100, 100))\n",
    "    images.append(img)\n",
    "    labels.append(label)\n",
    "\n",
    "  images = np.asarray(images)\n",
    "\n",
    "  return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e11c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only use 10% of the data\n",
    "\n",
    "covid_images, covid_labels = loadImages(covid_dir, os.listdir(covid_dir)[:int(0.1*len(os.listdir(covid_dir)))], 1)\n",
    "len(covid_images), len(covid_labels)\n",
    "\n",
    "normal_images, normal_labels = loadImages(normal_dir, os.listdir(normal_dir)[:int(0.1*len(os.listdir(normal_dir)))], 0)\n",
    "len(normal_images), len(normal_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c81178",
   "metadata": {},
   "outputs": [],
   "source": [
    "covid_images.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2285ae83",
   "metadata": {},
   "source": [
    "## Aufteilung der Daten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd576c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.r_[covid_images, normal_images]\n",
    "\n",
    "y = np.r_[covid_labels, normal_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a75b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec79c85",
   "metadata": {},
   "source": [
    "## Umwandlung der Bilder in Graphen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0b0089",
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_to_graph(image):\n",
    "  h, w, c = image.shape\n",
    "\n",
    "  node_indices = np.arange(h * w).reshape((h, w))\n",
    "\n",
    "  edges_r = np.stack((node_indices[:, :-1].reshape((-1)), node_indices[:, 1:].reshape((-1))), axis = 0)\n",
    "  edges_l = np.stack((node_indices[:, 1:].reshape((-1)), node_indices[:, :-1].reshape((-1))), axis = 0)\n",
    "  edges_d = np.stack((node_indices[:-1, :].reshape((-1)), node_indices[1:, :].reshape((-1))), axis = 0)\n",
    "  edges_u = np.stack((node_indices[1:, :].reshape((-1)), node_indices[:-1, :].reshape((-1))), axis = 0)\n",
    "\n",
    "  edges = np.concatenate((edges_r, edges_l, edges_d, edges_u), axis = 1).astype(np.int32)\n",
    "  edges = tf.convert_to_tensor(edges)\n",
    "\n",
    "  edge_weights = tf.ones(shape = edges.shape[1])\n",
    "\n",
    "  node_features = tf.convert_to_tensor(image.reshape((-1, 3)).astype(np.float32))\n",
    "\n",
    "  graph_info = (node_features, edges, edge_weights)\n",
    "\n",
    "  return graph_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daeddb3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_graphs = [image_to_graph(img) for img in x_train]\n",
    "\n",
    "test_graphs = [image_to_graph(img) for img in x_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "733e5889",
   "metadata": {},
   "source": [
    "## GNN-Architektur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f889de96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_fcnn(hidden_units):\n",
    "    fcnn_layers = []\n",
    "\n",
    "    for units in hidden_units:\n",
    "        fcnn_layers.append(layers.BatchNormalization())\n",
    "        fcnn_layers.append(layers.Dense(units, activation = tf.nn.relu))\n",
    "\n",
    "    return keras.Sequential(fcnn_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9537e627",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphConvLayer(layers.Layer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_units,\n",
    "        aggregation_type = \"mean\",\n",
    "        combination_type = \"concat\",\n",
    "        normalize = False,\n",
    "        *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "        self.aggregation_type = aggregation_type\n",
    "        self.combination_type = combination_type\n",
    "        self.normalize = normalize\n",
    "\n",
    "        self.fcnn_prepare = create_fcnn(hidden_units)\n",
    "        self.update_fn = create_fcnn(hidden_units)\n",
    "\n",
    "    def prepare(self, node_repesentations, weights = None):\n",
    "        b, num_edges, embedding_dim = node_repesentations.shape\n",
    "        messages = self.fcnn_prepare(node_repesentations)\n",
    "        if weights is not None:\n",
    "            messages = messages * tf.reshape(weights, (1, num_edges, 1))\n",
    "        \n",
    "        return messages\n",
    "\n",
    "    def aggregate(self, node_indices, neighbour_messages, node_repesentations):\n",
    "        num_nodes = node_repesentations.shape[1]\n",
    "        b, num_edges, embedding_dim = neighbour_messages.shape\n",
    "\n",
    "        neighbour_messages = tf.reshape(tf.transpose(neighbour_messages, (1, 0, 2)), (num_edges, -1))\n",
    "\n",
    "        aggregated_message = tf.math.unsorted_segment_mean(neighbour_messages, node_indices, num_segments = num_nodes)\n",
    "        aggregated_message = tf.reshape(aggregated_message, (num_nodes, -1, embedding_dim))\n",
    "        aggregated_message = tf.transpose(aggregated_message, (1, 0, 2))\n",
    "\n",
    "        return aggregated_message\n",
    "\n",
    "    def update(self, node_repesentations, aggregated_messages):\n",
    "        h = tf.concat([node_repesentations, aggregated_messages], axis = -1)\n",
    "        node_embeddings = self.update_fn(h)\n",
    "                    \n",
    "        return node_embeddings\n",
    "\n",
    "    def call(self, inputs):\n",
    "        node_repesentations, edges, edge_weights = inputs\n",
    "        node_indices, neighbour_indices = edges[0], edges[1]\n",
    "        neighbour_repesentations = tf.gather(node_repesentations, neighbour_indices, batch_dims=0, axis=1)\n",
    "\n",
    "        neighbour_messages = self.prepare(neighbour_repesentations, edge_weights)\n",
    "        aggregated_messages = self.aggregate(node_indices, neighbour_messages, node_repesentations)\n",
    "\n",
    "        return self.update(node_repesentations, aggregated_messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eeedcfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNNGraphClassifier(tf.keras.Model):\n",
    "    def __init__(\n",
    "        self,\n",
    "        graph_info,\n",
    "        hidden_units,\n",
    "        aggregation_type = \"sum\",\n",
    "        combination_type = \"concat\",\n",
    "        normalize = True,\n",
    "        *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "        node_features, edges, edge_weights = graph_info\n",
    "        self.node_features = node_features\n",
    "        self.edges = edges\n",
    "        self.edge_weights = edge_weights\n",
    "        self.edge_weights = self.edge_weights / tf.math.reduce_sum(self.edge_weights)\n",
    "\n",
    "        self.preprocess = create_fcnn(hidden_units)\n",
    "        self.conv1 = GraphConvLayer(hidden_units, aggregation_type, combination_type, normalize)\n",
    "        self.conv2 = GraphConvLayer(hidden_units, aggregation_type, combination_type, normalize)\n",
    "        self.postprocess = create_fcnn(hidden_units)\n",
    "        self.compute_sigmoid = layers.Dense(units = 1)\n",
    "\n",
    "    def call(self, node_features):\n",
    "        x = self.preprocess(node_features)\n",
    "        x1 = self.conv1((x, self.edges, self.edge_weights))\n",
    "        x = x1 + x\n",
    "        x2 = self.conv2((x, self.edges, self.edge_weights))\n",
    "        x = x2 + x\n",
    "        x = self.postprocess(x)\n",
    "\n",
    "        graph_embedding = tf.reduce_mean(x, axis = 1, keepdims = False)\n",
    "\n",
    "        return self.compute_sigmoid(graph_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a226e764",
   "metadata": {},
   "outputs": [],
   "source": [
    "gnn_model = GNNGraphClassifier(\n",
    "    graph_info = train_graphs[0],\n",
    "    hidden_units = [32])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf2788be",
   "metadata": {},
   "source": [
    "## Modelltraining und -evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4929338",
   "metadata": {},
   "outputs": [],
   "source": [
    "gnn_model.compile(optimizer = \"adam\",\n",
    "              loss = \"binary_crossentropy\",\n",
    "              metrics = \"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43ff9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.array([tg[0] for tg in train_graphs], dtype = np.float32)\n",
    "\n",
    "gnn_model.fit(x_train, y_train, epochs = 10, batch_size = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df56391",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = np.array([tg[0] for tg in test_graphs], dtype = np.float32),\n",
    "\n",
    "gnn_model.evaluate(x_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
